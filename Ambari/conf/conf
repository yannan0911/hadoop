# public
WORK_DIR=${WORK_DIR:-$SCRIPTS_DIR/..}
SCRIPTS_DIR=${SCRIPTS_DIR:-$WORK_DIR/scripts}
SCRIPTS_FLOW_DIR=${SCRIPTS_FLOW_DIR:-$WORK_DIR/scripts/flow}
CONF_DIR=${CONF_DIR:-$WORK_DIR/conf}
DATA_DIR=${DATA_DIR:-$WORK_DIR/data}
INPUT_DIR=${INPUT_DIR:-$WORK_DIR/input}
OUTPUT_DIR=${OUTPUT_DIR:-$WORK_DIR/output}
TMP_DIR=${TMP_DIR:-$WORK_DIR/tmp}
TMP_FLOW_DIR=${TMP_FLOW_DIR:-$TMP_DIR/flow}
LOG_DIR=${LOG_DIR:-$WORK_DIR/logs}

mkdir -p $DATA_DIR $INPUT_DIR $OUTPUT_DIR $TMP_FLOW_DIR $LOG_DIR

# ssh.sh
FAILED_LIST_TMP=${FAILED_LIST_TMP:-$TMP_DIR/failed_list}
PSWD_TMP=${PSWD_TMP:-$TMP_DIR/pswd}
LIST_INPUT=${LIST_INPUT:-$INPUT_DIR/list}

# init_host.sh
SEC1970=${SEC1970:-$(date +%s)}

HOSTS_INPUT=${INPUT_DIR}/hosts
USERADD_FILENAME='useradd_list'
USERADD_INPUT="${INPUT_DIR}/$USERADD_FILENAME"
NEW_LIST_INPUT=${NEW_LIST_INPUT:-$INPUT_DIR/new_list}
ALL_LIST_INPUT=${ALL_LIST_INPUT:-$INPUT_DIR/all_list}

HOME=${HOME:-/root}
SSH_KEY_DIR="$HOME/.ssh"
PRI_KEY_FILENAME='id_rsa'
PUB_KEY_FILENAME='id_rsa.pub'
PRI_KEY_FILE="$SSH_KEY_DIR/$PRI_KEY_FILENAME"
PUB_KEY_FILE="$SSH_KEY_DIR/$PUB_KEY_FILENAME"

JDK_DIR='/usr/local/jdk'

SSH_SH=$SCRIPTS_DIR/ssh.sh
SERVER_IP=${SERVER_IP:-''}
COMMAND=''

# mk_keys
MK_KEYS_SH=$SCRIPTS_DIR/mk_keys.sh
DEFAULT_REALM="@EXAMPLE.COM"
KEY_OUTPUT_DIR=$OUTPUT_DIR/keys
COM_KEY_OUTPUT_DIR=$KEY_OUTPUT_DIR/com

## api
EXCLUDED_HOSTS_INPUT=${EXCLUDED_HOSTS_INPUT:-$INPUT_DIR/excluded_hosts}

AMBARI_USER=${AMBARI_USER:-admin}
AMBARI_PASSWD=${AMBARI_PASSWD:-admin}
AMBARI_USER_PW=$AMBARI_USER:$AMBARI_PASSWD
AMBARI_SERVER=${AMBARI_SERVER:-server_url}        # rsync.landrover004.hadoop.sjs.ted
AMBARI_PORT=${AMBARI_PORT:-8080}
AMBARI_CLUSTER=${AMBARI_CLUSTER:-cluster_name}    # LandRover          
AMBARI_API="http://$AMBARI_SERVER:$AMBARI_PORT/api/v1/clusters/$AMBARI_CLUSTER"
AMBARI_REQUESTS="$AMBARI_API/requests"

## flow                     # 0.19 or 2.0
HDFS=${HDFS:-hdfs}          # /search/work/hadoop-envir/hadoop-namenode/bin/hadoop or hdfs
MR=${MR:-yarn}              # /search/work/hadoop-envir/hadoop-jobtracker/bin/hadoop or yarn

# check_dead_dn.sh
THRESHOLD_SECOND=${THRESHOLD_SECOND:-600}                               # datanode dead second threshold value
DFSADMIN_REPORT_TMP=${DFSADMIN_REPORT_TMP:-$TMP_DIR/dfsadmin_report}
DEAD_DN_LIST_TMP=${DEAD_DN_LIST_TMPFILE:-$TMP_FLOW_DIR/dead_dn_list}