# public
WORK_DIR=${WORK_DIR:-$SCRIPTS_DIR/..}
SCRIPTS_DIR=${SCRIPTS_DIR:-$WORK_DIR/scripts}
SCRIPTS_API_DIR=${SCRIPTS_API_DIR:-$WORK_DIR/scripts/api}
SCRIPTS_FLOW_DIR=${SCRIPTS_FLOW_DIR:-$WORK_DIR/scripts/flow}
CONF_DIR=${CONF_DIR:-$WORK_DIR/conf}
DATA_DIR=${DATA_DIR:-$WORK_DIR/data}
INPUT_DIR=${INPUT_DIR:-$WORK_DIR/input}
OUTPUT_DIR=${OUTPUT_DIR:-$WORK_DIR/output}
TMP_DIR=${TMP_DIR:-$WORK_DIR/tmp}
TMP_FLOW_DIR=${TMP_FLOW_DIR:-$TMP_DIR/flow}
LOG_DIR=${LOG_DIR:-$WORK_DIR/logs}

mkdir -p $DATA_DIR $INPUT_DIR $OUTPUT_DIR $TMP_FLOW_DIR $LOG_DIR

. $CONF_DIR/conf_special
# TODO: 增加配置文件及数据文件校对流程
HADOOP_CENTER_PUBKEY='ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAzz0XB9x1JWpacdzqoyqoHk9MI915+QslSbf5lt8LTy4S/dGeaHxpBctbjisGWHpS6KR4KNjvEbzq1ww0TWYUP6C7ib5k3NvnA4ERpuG1DzGXU1QCia3wndT6eOF+Yy0Eot6I4DHVPGN8kY9qikm2kRop5KZwOK/hWwTXp/ir+Gvdo0owfHipOZ5jv6XtUgOlcMcI/A+7iEphM6bMafoFHTkPQ6R80UTZfZH2ACTzSEil8EiaexZTJw+LXi8AQc7rcutax4JcUNMVuzWoFHHKYwRIoHzPvngN3bT/1WRUke5HzMcpI9Z2gjyzhEsAiYryg/UPlEARYPMsgpLpJUNLYQ== root@adtech-zabbix-server01'
SERVER_IP=${SERVER_IP:-$(/sbin/ip a | sed -rn '/scope global eth0/s/.*inet[[:blank:]]([0-9.]+)\/.*/\1/gp' | head -n 1)}
MAIL_TOOL=${MAIL_TOOL:-/opt/mail/sendmail.sh}
MAIL_TITAL="[Hadoop_Report] $HADOOP_PLATFORM-$SERVER_IP"
MAIL_CONTENT=''

SEC1970=${SEC1970:-$(date +%s)}

# ssh.sh
PSWD_TMP=$TMP_DIR/pswd
FAILED_LIST_TMP=$TMP_DIR/failed_list
LIST_INPUT=${LIST_INPUT:-$INPUT_DIR/list}

# init_host.sh
HOSTS_INPUT=$INPUT_DIR/hosts
USERADD_FILENAME='useradd_list'
USERADD_INPUT="$INPUT_DIR/$USERADD_FILENAME"
NEW_LIST_INPUT=${NEW_LIST_INPUT:-$INPUT_DIR/new_list}
ALL_LIST_INPUT=${ALL_LIST_INPUT:-$INPUT_DIR/all_list}
HADOOP_NODE_TGZ="hadoop-node_$HADOOP_PLATFORM.tgz"

HOME=${HOME:-/root}
SSH_KEY_DIR="$HOME/.ssh"
PRI_KEY_FILENAME='id_rsa'
PUB_KEY_FILENAME='id_rsa.pub'
PRI_KEY_FILE="$SSH_KEY_DIR/$PRI_KEY_FILENAME"
PUB_KEY_FILE="$SSH_KEY_DIR/$PUB_KEY_FILENAME"

JDK_DIR='/usr/local/jdk'

SSH_SH=$SCRIPTS_DIR/ssh.sh
COMMAND=''

# mk_keys
MK_KEYS_SH=$SCRIPTS_DIR/mk_keys.sh
DEFAULT_REALM="@EXAMPLE.COM"
KEY_OUTPUT_DIR=$OUTPUT_DIR/keys
COM_KEY_OUTPUT_DIR=$KEY_OUTPUT_DIR/com

## api
EXCLUDED_HOSTS_INPUT=${EXCLUDED_HOSTS_INPUT:-$INPUT_DIR/excluded_hosts}
MAINTENANCE_HOSTS_INPUT=${MAINTENANCE_HOSTS_INPUT:-$INPUT_DIR/maintenance_hosts}

AMBARI_USER_PW=$AMBARI_USER:$AMBARI_PASSWD
AMBARI_API="http://$AMBARI_SERVER:$AMBARI_PORT/api/v1/clusters/$AMBARI_CLUSTER"
AMBARI_REQUESTS="$AMBARI_API/requests"

## flow
# check_dead_dn.sh
THRESHOLD_SECOND=${THRESHOLD_SECOND:-600}                               # datanode dead second threshold value
DFSADMIN_REPORT_TMP=$TMP_DIR/dfsadmin_report
DEAD_DN_LIST_TMP=$TMP_FLOW_DIR/dead_dn_list

# backup_server
PG_DUMP=${PG_DUMP:-/usr/bin/pg_dump}
KDB5_UTIL=${KDB5_UTIL:-/usr/sbin/kdb5_util}

PLATFORM_TYPE=$(awk '/^## (1|2) ## '$HADOOP_PLATFORM'/{print $2}' $HOSTS_INPUT)
BACKUP_TIME=$(date -d"0 days ago" +"%Y%m%d%H")
DEL_DAY=$(date -d"2 days ago" +"%Y%m%d")
BACKUP_SERVER_DATA_DIR=${BACKUP_SERVER_DATA_DIR:-$SCRIPT_FLOW_BACKUP_SERVER_DIR/backup_data}
KEY_WORD_AMBARI_SERVER='org.apache.ambari.server.controller.AmbariServer'
KEY_WORD_KERBEROS='/usr/sbin/kadmind -P /var/run/kadmind.pid'
BACKUP_FILE_TAG=${BACKUP_FILE_TAG:-backup_ambari}

PG_USER_AMBARI=${PG_USER_AMBARI:-ambari}
PG_DATABASE_AMBARI=${PG_DATABASE_AMBARI:-ambari}
PG_USER_AMBARIRCA=${PG_USER_AMBARIRCA:-mapred}
PG_DATABASE_AMBARIRCA=${PG_DATABASE_AMBARIRCA:-ambarirca}

KRB_DUMP_FILE=${KRB_DUMP_FILE:-${BACKUP_FILE_TAG}_kerberos_dump_file}
KRB_DUMP_FILE_OK=$KRB_DUMP_FILE.dump_ok
SYS_VAR_DIR='/var'
KRB_DIR_NAME=${KRB_DIR_NAME:-kerberos}
KRB_ROOT_DIR=${KRB_ROOT_DIR:-$SYS_VAR_DIR/$KRB_DIR_NAME}
KRB_ETC_CONF=${KRB_ETC_CONF:-/etc/krb5.conf}

# check_edits
EDITS_FILE_DIR=${EDITS_DIR:-/search/work/hadoop-envir/hadoop-data/dfs/name/current}
EDITS_FILE_SIZE_THRESHOLD=209715200    # 200MB

# hadoop_job_time
JOB_TIME_THRESHOLD_TIMEOUT_MINUTES=${JOB_TIME_THRESHOLD_TIMEOUT_MINUTES:-120}    #min
JOB_TIME_THRESHOLD_OFFSET_MINUTES=$(($JOB_TIME_THRESHOLD_TIMEOUT_MINUTES+3))    #min
JOB_TIME_THRESHOLD_OFFSET_SECOND=$((JOB_TIME_THRESHOLD_OFFSET_MINUTES*60))     #second
JOB_TIME_THRESHOLD_MAPNUM=30000
JOB_TIME_HOUR_START=22
JOB_TIME_HOUR_END=10
DATE_MIN_AGO_YMD=$(date -d "$JOB_TIME_THRESHOLD_OFFSET_MINUTES minutes ago" +"%Y%m%d")
TIME_HMS=$(date +"%H%M%S")
NOW_SECOND=$(date +"%s")
NOW_MIN=$(date +"%M")
NOW_HOUR=$(date +"%H")
NOW_DATE=$(date +"%Y%m%d")
NOW_YEAR=$(date +"%Y")
NOW_MONTH=$(date +"%m")
NOW_DAY=$(date +"%d")
YEAR_30DAYS_AGO=$(date -d'30 days ago' +"%Y")
MONTH_30DAYS_AGO=$(date -d'30 days ago' +"%m")
DAY_30DAYS_AGO=$(date -d'30 days ago' +"%d")

NOW_YMD_DIR=$NOW_YEAR/$NOW_MONTH/$NOW_DAY
DEL_YMD_DIR=$YEAR_30DAYS_AGO/$MONTH_30DAYS_AGO/$DAY_30DAYS_AGO

JOB_HISTORY_LOG_ROOT_DIR="$JOBTRACKER_DIR/logs/history"
JOB_HISTORY_LOG_DIR="$JOB_HISTORY_LOG_ROOT_DIR/$DATE_MIN_AGO_YMD"

RANDOM_NUM=$RANDOM

WHITE_JOBNAME_LIST="white_jobname.list"
WHITE_JOBUSER_LIST="white_jobuser.list"
BLACK_JOBNAME_LIST="black_jobname.list"
WHITE_JOBNAME_MAPNUM_LIST="white_jobname_mapnum.list"
ALL_JOB_FILE="job_list"
KILL_JOB_FILE="job_to_kill"
ALL_JOB_LIST="logs/$NOW_YMD_DIR/$ALL_JOB_FILE.${NOW_DATE}_${TIME_HMS}.$RANDOM_NUM"
KILL_JOB_LIST="logs/$NOW_YMD_DIR/$KILL_JOB_FILE.${NOW_DATE}_${TIME_HMS}.$RANDOM_NUM"

## import func
. $SCRIPTS_DIR/func.sh
